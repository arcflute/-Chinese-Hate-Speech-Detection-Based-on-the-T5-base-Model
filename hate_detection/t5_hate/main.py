import json
import torch
import jieba
import synonyms
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    EarlyStoppingCallback,
)
from datasets import Dataset
from tqdm import tqdm



# ========== EDA å¢å¼ºå‡½æ•° ==========
def get_synonyms(word: str, topk: int = 5):
    syns = synonyms.nearby(word)[0]
    return [w for w in syns if w != word][:topk]

def synonym_replacement(words, n_sr=1):
    new_words = words.copy()
    candidates = [w for w in words if get_synonyms(w)]
    for w in random.sample(candidates, min(n_sr, len(candidates))):
        idx = new_words.index(w)
        new_words[idx] = random.choice(get_synonyms(w))
    return new_words

def random_insertion(words, n_ri=1):
    new_words = words.copy()
    for _ in range(n_ri):
        candidates = [w for w in new_words if get_synonyms(w)]
        if not candidates: break
        w = random.choice(candidates)
        new_words.insert(random.randint(0, len(new_words)), random.choice(get_synonyms(w)))
    return new_words

def random_swap(words, n_rs=1):
    new_words = words.copy()
    for _ in range(n_rs):
        if len(new_words) < 2: break
        i, j = random.sample(range(len(new_words)), 2)
        new_words[i], new_words[j] = new_words[j], new_words[i]
    return new_words

def random_deletion(words, p=0.1):
    if len(words) == 1: return words
    new = [w for w in words if random.random() > p]
    return new if new else [random.choice(words)]

def eda(sentence: str,
        alpha_sr=0.1, alpha_ri=0.1,
        alpha_rs=0.1, p_rd=0.1,
        num_aug=4):
    words = list(jieba.cut(sentence, cut_all=False))
    n = len(words)
    n_sr = max(1, int(alpha_sr * n))
    n_ri = max(1, int(alpha_ri * n))
    n_rs = max(1, int(alpha_rs * n))
    
    augmented = []
    augmented.append("".join(synonym_replacement(words, n_sr)))
    augmented.append("".join(random_insertion(words, n_ri)))
    augmented.append("".join(random_swap(words, n_rs)))
    augmented.append("".join(random_deletion(words, p_rd)))
    
    for _ in range(num_aug - 4):
        op = random.choice([
            lambda w: synonym_replacement(w, n_sr),
            lambda w: random_insertion(w, n_ri),
            lambda w: random_swap(w, n_rs),
            lambda w: random_deletion(w, p_rd),
        ])
        augmented.append("".join(op(words)))
    return augmented

#  æ¨¡å‹é…ç½®
model_name = "Langboat/mengzi-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

#  å¼ºåˆ¶ä½¿ç”¨GPU
if not torch.cuda.is_available():
    raise RuntimeError("å¿…é¡»ä½¿ç”¨GPUè®­ç»ƒï¼Œè¯·æ£€æŸ¥CUDAç¯å¢ƒ")
device = torch.device("cuda")
model = model.to(device)

#  åŠ è½½ JSON æ•°æ®
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

#  æ¸…æ´—æ•°æ®ï¼ˆä»…ä¿ç•™æœ‰ content å’Œéç©º output çš„æ ·æœ¬ï¼‰
def clean_data(data):
    return [d for d in data if d.get("content") and d.get("output") and d["output"].strip()]

#  åŠ è½½å¹¶å¤„ç†è®­ç»ƒæ•°æ®
all_data = clean_data(load_json("../train.json"))

# ========== åœ¨è¿™é‡Œåš EDA å¢å¼º ==========
augmented = []
for item in all_data:
    aug_texts = eda(
        item["content"],
        alpha_sr=0.1, alpha_ri=0.1,
        alpha_rs=0.1, p_rd=0.1,
        num_aug=4
    )
    for txt in aug_texts:
        augmented.append({"content": txt, "output": item["output"]})
# åˆå¹¶åŸå§‹å’Œå¢å¼ºæ•°æ®
all_data = all_data + augmented

#  è‡ªåŠ¨åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ9:1ï¼‰
train_data, val_data = train_test_split(all_data, test_size=0.1, random_state=42)

#  åŠ è½½æµ‹è¯•é›†
test_data = load_json("../test1.json")

#  è½¬æ¢ä¸º Huggingface Datasets
train_ds = Dataset.from_list(train_data)
val_ds = Dataset.from_list(val_data)
test_ds = Dataset.from_list(test_data)

#  åˆ†è¯å‡½æ•°
def tokenize_function(example):
    model_inputs = tokenizer(
        example["content"],
        max_length=256,
        padding="max_length",
        truncation=True,
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example["output"],
            max_length=256,
            padding="max_length",
            truncation=True,
        )
    model_inputs["labels"] = [
        (token if token != tokenizer.pad_token_id else -100)
        for token in labels["input_ids"]
    ]
    return model_inputs

# åˆ†è¯æ˜ å°„
tokenized_train = train_ds.map(tokenize_function)
tokenized_val = val_ds.map(tokenize_function)

# æ•°æ®æ‰“åŒ…å™¨
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# è®­ç»ƒå‚æ•°
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="steps",
    eval_steps=100,
    logging_steps=50,
    save_steps=100,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    save_total_limit=2,
    predict_with_generate=True,
    fp16=True,  
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
)

# åˆå§‹åŒ– Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

# ä¿å­˜æ¨¡å‹
model.save_pretrained("../saved_model")
tokenizer.save_pretrained("../saved_model")


print("ğŸ” å¼€å§‹æ¨ç†æµ‹è¯•é›†...")

model.eval()
with open("demo.txt", "w", encoding="utf-8") as f:
    for item in tqdm(test_data):
        input_text = item["content"]
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            max_length=256,
            truncation=True,
            padding="max_length"
        ).to(device)

        output_ids = model.generate(
            **inputs,
            max_length=256,
            num_beams=5,
            early_stopping=True
        )
        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

 
        if not pred.endswith("[END]"):
            pred += " [END]"

        f.write(pred + "\n")
